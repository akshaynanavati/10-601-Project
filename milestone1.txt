Kaggle team name:  Folk Dance
Andrew IDs of team: ananavat, shubhits

For classifier one, we ran the pruned C4.5 decision tree classifier from Weka 3.6.11 (Java class weka.classifiers.trees.J48) on the full training dataset, using a pruning confidence of 0.35 and a minimum of 2 instances per leaf (options "-C 0.35 -M 2") and using the default options otherwise. Training took just under 2 minutes and obtained an accuracy of 92.225% on the training set and 21.676% on the testing set. Most of the errors in the training set were between similar looking images, such as birds and airplanes, and automobiles, trucks, and horses. The test data accuracy is very low, and this is probably due to some overfitting of the training data and the absence of any useful feature extraction steps in the algorithm. The corresponding submission for this classifier on Kaggle is entitled 'MS1Classifier1Submission.csv'.  

For classifier two, we ran a Conjunctive Rule learner (Java class weka.classifiers.rules.ConjunctiveRule) on the full training data set with the default parameters. We chose to use a rule based learner just to see how well it would perform. It got an accuracy of 0.10381 on the testing set (after submitting to Kaggle). We believe it is this low because we did no feature selection or pre-processing of the data. RGB is probably not the best feature vector to be looking at because two pictures with the same object could have totally different colors. Therefore, modifying the features to another color space, or using clustering to change the features before running a classifier might lead to better results

